# -*- coding: utf-8 -*-
"""Test_LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UMkaNCMtFdWd56YjkJcCaKl7IjkyYozV
"""

!pip install --upgrade pip
!pip install --upgrade bitsandbytes transformers accelerate peft datasets
!pip install Mistral

from google.colab import drive
drive.mount('/content/drive')

BASE = "/content/drive/MyDrive/Colab_Notebooks/LoRA"
DATA_DIR = f"{BASE}/datasets"
OUT_DIR  = f"{BASE}/outputs_lora_4bit"

from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
from google.colab import userdata
HF_TOKEN=userdata.get('HF_TOKEN')

login(HF_TOKEN)
model_id = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer = AutoTokenizer.from_pretrained(model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

import torch
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
model.config.pad_token_id = tokenizer.pad_token_id

from datasets import load_dataset

DATA_DIR = "/content/drive/MyDrive/Colab_Notebooks/LoRA/datasets"

train_file = f"{DATA_DIR}/train_lyra_immo_mistral.jsonl"
valid_file = f"{DATA_DIR}/validation_lyra_immo_mistral.jsonl"

train_ds = load_dataset("json", data_files=train_file, split="train")
valid_ds = load_dataset("json", data_files=valid_file, split="train")

print(train_ds[0])

max_len = 512  # safe pour commencer

def tok_fn(ex):
    tok = tokenizer(
        ex["text"],
        truncation=True,
        max_length=max_len,
        padding=False
    )
    tok["labels"] = tok["input_ids"].copy()
    return tok

tok_train = train_ds.map(tok_fn, batched=True, remove_columns=["text"])
tok_valid = valid_ds.map(tok_fn, batched=True, remove_columns=["text"])

from peft import LoraConfig, get_peft_model

lora_cfg = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "down_proj"]
)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=10,
    save_strategy="epoch",
    save_total_limit=1,
    bf16=torch.cuda.is_bf16_supported(),
    fp16=not torch.cuda.is_bf16_supported(),
    disable_tqdm=False
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_train,
    eval_dataset=tok_valid,
    data_collator=collator
)

trainer.train()

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# üìÇ chemins
base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"
lora_dir = "/content/drive/MyDrive/Colab_Notebooks/LoRA/outputs/checkpoint-225"  # adapte au bon checkpoint

# üîπ Charger mod√®le de base
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
base_model.config.pad_token_id = tokenizer.pad_token_id

# üîπ Charger mod√®le LoRA
lora_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)
lora_model = PeftModel.from_pretrained(lora_model, lora_dir)
lora_model.config.pad_token_id = tokenizer.pad_token_id


# üîπ Prompt syst√®me (invariable)
system_prompt = (
    "Tu es un expert en estimation immobili√®re. "
    "R√©ponds en fran√ßais, de mani√®re concise, en 1 √† 2 phrases. "
    "Donne une estimation chiffr√©e (fourchette en euros) et un bref commentaire."
    "les localisations sont class√©es par zone, elles sont de type A, B1, B2 ou C"
    "A = tr√®s demand√©, B1 et B2 = un peu moins demand√©, C = les moins demand√©es"
    "Ne mentionne jamais de villes, seulement les zones."
)

# üîπ Prompts utilisateur (param√®tres)
user_prompt = [
    "Surface : 96 m¬≤\nType : maison\n√âtat : bon\nLocalisation :  B1",
    "Surface : 55 m¬≤\nType : appartement\n√âtat : √† r√©nover\nLocalisation : C",
    "Surface : 120 m¬≤\nType : maison\n√âtat : excellent\nLocalisation : A"
]

# üîπ Fonction pour combiner syst√®me + utilisateur
def build_prompt(user_prompt):
    return f"{system_prompt}\n\n{user_prompt}"

# --- Bloc utilitaire : g√©n√©ration + coupe propre ---
def generate_answer(model, tokenizer, prompt,
                    max_new_tokens=120,
                    do_sample=False,
                    temperature=0.2,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    stop_tokens=None):
    """
    G√©n√®re une r√©ponse courte et coupe sur des tokens d'arr√™t simples.
    - do_sample=False => greedy (stable). Pour √©chantillonnage, mets do_sample=True.
    """
    stop_tokens = stop_tokens or ["\n#", "##", "###", "USER:", "SYSTEM:"]

    model.eval()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    gen_kwargs = dict(
        max_new_tokens=max_new_tokens,
        do_sample=do_sample,
        repetition_penalty=repetition_penalty,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )
    if do_sample:
        gen_kwargs.update(temperature=temperature, top_p=top_p)

    with torch.no_grad():
        out = model.generate(**inputs, **gen_kwargs)

    text = tokenizer.decode(out[0], skip_special_tokens=True)

    # Si le mod√®le r√©-√©chote le prompt, on ne garde que la suite
    if text.startswith(prompt):
        text = text[len(prompt):].lstrip()

    # Coupe sur le premier stop token trouv√©
    for s in stop_tokens:
        idx = text.find(s)
        if idx != -1:
            text = text[:idx].rstrip()
            break

    return text

# --- Test des prompts ---
for i, p in enumerate(user_prompt, 1):
    full_prompt = build_prompt(p)

    print(f"\n--- Prompt {i} ---\n{p}\n")

    print("‚û°Ô∏è Base model:")
    print(generate_answer(base_model, tokenizer, full_prompt))

    print("\n‚û°Ô∏è LoRA model:")
    print(generate_answer(lora_model, tokenizer, full_prompt))

    print("="*120)

# Fusionner LoRA ‚Üí mod√®le complet standalone
merged_model = model.merge_and_unload()

# Sauvegarde locale
save_dir = "/content/merged_mistral7b_estimation"
merged_model.save_pretrained(save_dir, safe_serialization=True)
tokenizer.save_pretrained(save_dir)

from huggingface_hub import login
login(HF_TOKEN)  # ton token HF

repo_id = "jeromex1/Lyra-Mistral7B-immobilier-LoRA"
merged_model.push_to_hub(repo_id)
tokenizer.push_to_hub(repo_id)

